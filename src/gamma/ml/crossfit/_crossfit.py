"""
Core implementation of :mod:`gamma.ml.crossfit`
"""
import logging
from abc import ABC
from typing import *

import pandas as pd
from sklearn.model_selection import BaseCrossValidator

from gamma.common.fit import FittableMixin
from gamma.common.parallelization import ParallelizableMixin
from gamma.ml import Sample
from gamma.sklearndf import BaseEstimatorDF, BaseLearnerDF, ClassifierDF, RegressorDF

log = logging.getLogger(__name__)

__all__ = ["BaseCrossfit", "LearnerCrossfit"]

T = TypeVar("T")
T_EstimatorDF = TypeVar("T_EstimatorDF", bound=BaseEstimatorDF)
T_LearnerDF = TypeVar("T_LearnerDF", bound=BaseLearnerDF)
T_ClassifierDF = TypeVar("T_ClassifierDF", bound=ClassifierDF)
T_RegressorDF = TypeVar("T_RegressorDF", bound=RegressorDF)


class BaseCrossfit(
    FittableMixin[Sample], ParallelizableMixin, ABC, Generic[T_EstimatorDF]
):
    """
    Fits an estimator to all train splits of a given cross-validation strategy.

    :param base_estimator: predictive pipeline to be fitted
    :param cv: the cross validator generating the train splits
    :param n_jobs: number of jobs to use in parallel; \
        if `None`, use joblib default (default: `None`).
    :param shared_memory: if `True` use threads in the parallel runs. If `False` \
        use multiprocessing (default: `False`).
    :param pre_dispatch: number of batches to pre-dispatch; \
        if `None`, use joblib default (default: `None`).
    :param verbose: verbosity level used in the parallel computation; \
        if `None`, use joblib default (default: `None`).
    """

    __slots__ = [
        "base_estimator",
        "cv",
        "n_jobs",
        "shared_memory",
        "verbose",
        "_model_by_split",
    ]

    def __init__(
        self,
        base_estimator: T_EstimatorDF,
        cv: BaseCrossValidator,
        n_jobs: Optional[int] = None,
        shared_memory: Optional[bool] = None,
        pre_dispatch: Optional[Union[str, int]] = None,
        verbose: Optional[int] = None,
    ) -> None:
        super().__init__(
            n_jobs=n_jobs,
            shared_memory=shared_memory,
            pre_dispatch=pre_dispatch,
            verbose=verbose,
        )
        self.base_estimator = base_estimator
        self.cv = cv

        self._model_by_split: Optional[List[T_EstimatorDF]] = None
        self._training_sample: Optional[Sample] = None

    def fit(self: T, sample: Sample, **fit_params) -> T:
        """
        Fit the base estimator to the full sample, and fit a clone of the base
        estimator to each of the train splits generated by the cross-validator
        :param sample: the sample to fit the estimators to
        :param fit_params: optional fit parameters, to be passed on to the fit method \
            of the base estimator
        :return: `self`
        """
        self_typed: BaseCrossfit = self  # support better type hinting in PyCharm
        base_estimator = self_typed.base_estimator

        features = sample.features
        target = sample.target

        base_estimator.fit(X=sample.features, y=sample.target, **fit_params)

        train_splits, test_splits = tuple(zip(*self_typed.cv.split(features, target)))

        with self_typed._parallel() as parallel:
            self._model_by_split: List[T_EstimatorDF] = parallel(
                self_typed._delayed(BaseCrossfit._fit_model_for_split)(
                    base_estimator.clone(),
                    features.iloc[train_indices],
                    target.iloc[train_indices],
                    **fit_params,
                )
                for train_indices in train_splits
            )

        self_typed._training_sample = sample

        return self

    @property
    def is_fitted(self) -> bool:
        """`True` if the delegate estimator is fitted, else `False`"""
        return self._training_sample is not None

    def get_n_splits(self) -> int:
        """
        Number of splits used for this crossfit.
        """
        self._ensure_fitted()
        return len(self._model_by_split)

    def splits(self) -> Iterator[Tuple[Sequence[int], Sequence[int]]]:
        self._ensure_fitted()
        return self.cv.split(
            self._training_sample.features, self._training_sample.target
        )

    def models(self) -> Iterator[T_EstimatorDF]:
        """Iterator of all models fitted on the cross-validation train splits."""
        self._ensure_fitted()
        return iter(self._model_by_split)

    @property
    def training_sample(self) -> Sample:
        """The sample used to train this crossfit."""
        self._ensure_fitted()
        return self._training_sample

    # noinspection PyPep8Naming
    @staticmethod
    def _fit_model_for_split(
        estimator: T_EstimatorDF,
        X: pd.DataFrame,
        y: Union[pd.Series, pd.DataFrame],
        **fit_params,
    ) -> T_EstimatorDF:
        """
        Fit a pipeline using a sample.

        :param estimator:  the :class:`gamma.ml.ModelPipelineDF` to fit
        :param train_sample: data used to fit the pipeline
        :return: fitted pipeline for the split
        """
        return estimator.fit(X=X, y=y, **fit_params)


class LearnerCrossfit(BaseCrossfit[T_LearnerDF], ABC, Generic[T_LearnerDF]):
    """
    Generate cross-validated prediction for each observation in a sample, based on
    multiple fits of a learner across a collection of cross-validation splits

    :param base_estimator: predictive pipeline to be fitted
    :param cv: the cross validator generating the train splits
    :param n_jobs: number of jobs to _rank_learners in parallel (default: 1)
    :param shared_memory: if ``True`` use threading in the parallel runs. If `False`, \
      use multiprocessing
    :param verbose: verbosity level used in the parallel computation
    """

    COL_SPLIT_ID = "split_id"
    COL_TARGET = "target"

    def __init__(
        self,
        base_estimator: T_LearnerDF,
        cv: BaseCrossValidator,
        *,
        n_jobs: Optional[int] = None,
        shared_memory: Optional[bool] = None,
        pre_dispatch: Optional[Union[str, int]] = None,
        verbose: Optional[int] = None,
    ) -> None:
        super().__init__(
            base_estimator=base_estimator,
            cv=cv,
            n_jobs=n_jobs,
            shared_memory=shared_memory,
            pre_dispatch=pre_dispatch,
            verbose=verbose,
        )
